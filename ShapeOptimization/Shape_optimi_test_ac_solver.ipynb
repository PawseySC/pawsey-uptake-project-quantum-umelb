{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d82bbe3",
   "metadata": {},
   "source": [
    "# Airfoil optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e81e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import distributions\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "\n",
    "#stuff for neurofoil - pip install neuralfoil\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import contextlib\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "import pennylane as qml\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, Lock, Value\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc08e1-e5dd-427b-8cd3-b4a5e267f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = 1 \n",
    "glock = mp.Lock()\n",
    "\n",
    "def get_id(lock):\n",
    "    global id_\n",
    "    with lock:\n",
    "        id_ += 1\n",
    "    return id_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94ef23",
   "metadata": {},
   "source": [
    "## Procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df10d0d",
   "metadata": {},
   "source": [
    "Sources:\n",
    "* http://servidor.demec.ufpr.br/CFD/bibliografia/aerodinamica/kulfan_2007.pdf\n",
    "* https://www.tandfonline.com/doi/epdf/10.1080/19942060.2024.2445144\n",
    "* https://github.com/peterdsharpe/NeuralFoil\n",
    "\n",
    "\n",
    "Here the PINN is allready trained an provides the prediction. Therefore, the idea is, we shift the quantum opt. part to approximate a policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db3dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SVG(filename=\"optimprocedure.svg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3325b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"../blade_design_tools\"))\n",
    "import create_airfoil as ca\n",
    "airfoil_top_points, airfoil_bottom_points, camber_curve_points = ca.generate_shape(config_filepath=\"../blade_design_tools/airfoil_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98572a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified shape_method - need controled file write!\n",
    "def generate_shape(config_filepath=\"airfoil_config.yaml\", config_dict=None, plot_foil=False):\n",
    "    try:\n",
    "        config = ca.load_config(config_filepath) if config_dict is None else config_dict\n",
    "        \n",
    "        # Handle optional input targets for chord and max thickness\n",
    "        input_targets = config.get('input_targets', {})\n",
    "        desired_chord = input_targets.get('desired_chord_length', None)\n",
    "        desired_max_th = input_targets.get('desired_max_thickness', None)\n",
    "        \n",
    "        # Temporarily compute camber to get normalized chord (needed for adjustments)\n",
    "        temp_camber_control_points, temp_camber_curve_points = ca.compute_curve_points(\n",
    "            config['camber_line'], \"camber\", config['output_settings']['num_points_on_curve']\n",
    "        )\n",
    "        norm_le = temp_camber_curve_points[0]\n",
    "        norm_te = temp_camber_curve_points[-1]\n",
    "        norm_chord = np.linalg.norm(norm_te - norm_le)\n",
    "        norm_x_span = norm_te[0] - norm_le[0]  # Should be 1.0\n",
    "        \n",
    "        # Adjust chord_length_for_export if desired_chord is set (to achieve desired geometric chord)\n",
    "        if desired_chord is not None:\n",
    "            config['output_settings']['chord_length_for_export'] = desired_chord * (norm_x_span / norm_chord)  # Equivalent to desired_chord * np.cos(np.deg2rad(np.abs(config['camber_line']['stagger_angle_deg'])))\n",
    "        \n",
    "        # 1. Compute Camber Line\n",
    "        camber_control_points, camber_curve_points = ca.compute_curve_points(\n",
    "            config['camber_line'], \"camber\", config['output_settings']['num_points_on_curve']\n",
    "        )\n",
    "       \n",
    "        # 2. Compute Top Thickness Distribution\n",
    "        top_thickness_control_points, top_thickness_curve_points = ca.compute_curve_points(\n",
    "            config['top_thickness'], \"top_thickness\", config['output_settings']['num_points_on_curve']\n",
    "        )\n",
    "        # 3. Compute Bottom Thickness Distribution\n",
    "        bottom_thickness_control_points, bottom_thickness_curve_points = ca.compute_curve_points(\n",
    "            config['bottom_thickness'], \"bottom_thickness\", config['output_settings']['num_points_on_curve']\n",
    "        )\n",
    "        \n",
    "        # Temporarily compute current normalized max thickness\n",
    "        current_norm_thickness_along = top_thickness_curve_points[:, 1] + bottom_thickness_curve_points[:, 1]\n",
    "        current_norm_max_th = np.max(current_norm_thickness_along)\n",
    "        \n",
    "        # Compute scale factor (reflects any chord adjustment)\n",
    "        scale = config['output_settings']['chord_length_for_export'] / norm_x_span\n",
    "        \n",
    "        # Adjust thickness curves if desired_max_th is set\n",
    "        if desired_max_th is not None:\n",
    "            desired_norm_max_th = desired_max_th / scale\n",
    "            th_scale_factor = desired_norm_max_th / current_norm_max_th\n",
    "            top_thickness_curve_points[:, 1] *= th_scale_factor\n",
    "            bottom_thickness_curve_points[:, 1] *= th_scale_factor\n",
    "        \n",
    "        # 4. Generate Full Airfoil Contour\n",
    "        airfoil_top_points, airfoil_bottom_points = ca.generate_airfoil_contour(\n",
    "            camber_curve_points,\n",
    "            top_thickness_curve_points,\n",
    "            bottom_thickness_curve_points,\n",
    "            config['camber_line']['inlet_angle_deg'],\n",
    "            config['output_settings']['num_points_on_arc']\n",
    "        )\n",
    "        # 5. Plot the Airfoil\n",
    "        if plot_foil:\n",
    "            ca.plot_airfoil(camber_curve_points, airfoil_top_points, airfoil_bottom_points,\n",
    "                         config['output_settings']['plot_title'])\n",
    "\n",
    "        if config['output_settings']['output_airfoil_filename'] != \"None\":\n",
    "            ca.write_airfoil_dat(airfoil_top_points, airfoil_bottom_points, camber_curve_points,\n",
    "                            config['output_settings']['output_airfoil_filename'],\n",
    "                            config['output_settings']['chord_length_for_export'])\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"{e}\")\n",
    "        pass\n",
    "       \n",
    "    return airfoil_top_points, airfoil_bottom_points, camber_curve_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with to adjustable params:\n",
    "# NetworkA -> ChordLength + MaxThickness - using black box function to translate this into our parametrization\n",
    "config_ = ca.load_config(\"../blade_design_tools/airfoil_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9237ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12575eb1-3392-4421-85d2-ece281dddf11",
   "metadata": {},
   "source": [
    "# Actor crtitic training\n",
    "- For the purpose of a prototype, we start modifying a few control points\n",
    "```python\n",
    "camber_line:\n",
    "  control_points_params:\n",
    "    P1_k_factor: 0.4\n",
    "    P2_x: 0.5\n",
    "    P2_y: 0.05\n",
    "    P3_k_factor: -0.25\n",
    "  inlet_angle_deg: 45\n",
    "  outlet_angle_deg: -50\n",
    "  stagger_angle_deg: -25\n",
    "\n",
    "top_thickness:\n",
    "  control_points_params:\n",
    "    P1_k_factor: 0.2\n",
    "    P2_x: 0.1\n",
    "    P2_y: 0.03\n",
    "    P3_k_factor: 0\n",
    "  inlet_angle_deg: 40\n",
    "  outlet_angle_deg: -0\n",
    "  le_y_thickness: 0.02 # Y-coordinate of P0 for thickness curve (half-thickness at LE)\n",
    "  p4_y: 0.01           # Y-coordinate of P4 for thickness curve (half-thickness at TE)\n",
    "\n",
    "bottom_thickness:\n",
    "  control_points_params:\n",
    "    P1_k_factor: 0.2\n",
    "    P2_x: 0.1\n",
    "    P2_y: 0.03\n",
    "    P3_k_factor: 0\n",
    "  inlet_angle_deg: 40\n",
    "  outlet_angle_deg: -0\n",
    "  le_y_thickness: 0.02 # Y-coordinate of P0 for thickness curve (half-thickness at LE)\n",
    "  p4_y: 0.01           # Y-coordinate of P4 for thickness curve (half-thickness at TE)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a94f4-0862-4d54-8148-ee8f1453fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_idx = {\n",
    "    1: [\"camber_line\",\"control_points_params\",\"P1_k_factor\",(0.15,0.5)],\n",
    "    2: [\"camber_line\",\"control_points_params\",\"P3_k_factor\",(-0.3,-0.1)],\n",
    "    3: [\"top_thickness\",\"control_points_params\",\"P1_k_factor\",(0.15,0.4)],\n",
    "    4: [\"top_thickness\",\"le_y_thickness\",(0.01,0.04)]\n",
    "}\n",
    "\n",
    "control_params = {\n",
    "    \"top_thickness\" : \"bottom_thickness\"\n",
    "}\n",
    "\n",
    "qoi = \"pitch2\"\n",
    "\n",
    "def run_script(script_path):\n",
    "    result = subprocess.run(\n",
    "        [script_path],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "        shell=True  # only needed if script is not executable\n",
    "    )\n",
    "    return result.stdout, result.stderr, result.returncode\n",
    "\n",
    "def fail_ret(def_path):\n",
    "    os.chdir(def_path)\n",
    "    return torch.Tensor([[-1]]), torch.Tensor([[0]])\n",
    "\n",
    "@torch.no_grad()\n",
    "def external_eval(actions_, run_id, plot_foil=False):  # action trajectoroy consist of max 4\n",
    "    actions = actions_.numpy()[0]\n",
    "    \n",
    "    config = copy.copy(config_)    \n",
    "    for step, action_ in enumerate(actions):\n",
    "        action_data = action_idx[step+1]\n",
    "        lb = action_data[-1][0]\n",
    "        ub = action_data[-1][1]\n",
    "\n",
    "        if len(action_data) == 4:\n",
    "            config[action_data[0]][action_data[1]][action_data[2]] = lb + action_ * (ub- lb)\n",
    "    \n",
    "    \n",
    "        #for simplicity just consider one thick. param\n",
    "        if len(action_data) == 3:\n",
    "            config[action_data[0]][action_data[1]] = lb + action_ * (ub - lb)\n",
    "            config[control_params[action_data[0]]][action_data[1]] = lb + action_ * (ub - lb)\n",
    "\n",
    "    config['output_settings']['output_airfoil_filename'] = f\"airfoil{run_id}.dat\"\n",
    "    airfoil_top_points, airfoil_bottom_points, _ = generate_shape(config_dict=config, plot_foil=plot_foil)\n",
    "\n",
    "    #cp test dummy stuff    \n",
    "    new_f = f\"test{run_id}\"\n",
    "    cwd = os.getcwd()\n",
    "    os.system(f\"cp -r uptake/dummy uptake/{new_f}\")\n",
    "    os.system(f\"mv airfoil{run_id}.dat uptake/{new_f}/airfoil.dat\")\n",
    "    os.chdir(os.path.join(\"uptake\",new_f))\n",
    "\n",
    "    stdout, stderr, ret = run_script(\"../scripts/build.sh\")\n",
    "    if ret != 0:\n",
    "        print(f\"Build script failed with code {ret}\")\n",
    "        return fail_ret(cwd)\n",
    "        \n",
    "    stdout, stderr, ret = run_script(\"../scripts/run_traf.sh\")\n",
    "    if ret != 0:\n",
    "        print(f\"Run script failed with code {ret}\")\n",
    "        return fail_ret(cwd)\n",
    "    \n",
    "    if not os.path.exists(\"fort.40\"):\n",
    "        return fail_ret(cwd)\n",
    "    \n",
    "    stdout, stderr, ret = run_script(\"../scripts/do_post.sh\")\n",
    "    \n",
    "    #calc loss\n",
    "    try:\n",
    "        y,pt = np.genfromtxt(qoi,skip_header=3,usecols=[0,5]).T\n",
    "        loss = 1-np.trapz(pt,y)    \n",
    "    except:\n",
    "        return fail_ret(cwd)\n",
    "    \n",
    "    os.chdir(cwd)\n",
    "    print(f\"Run_id: {run_id}\")\n",
    "    \n",
    "    return torch.Tensor([[1/loss]]), torch.Tensor([[loss]]), run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fa464-0692-4fe7-b97d-87e890e6d25e",
   "metadata": {},
   "source": [
    "# Refine AC with PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc117e-6540-4561-8333-aa9f5b50636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, n_numbers, hidden_dim=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_numbers * 2)\n",
    "        )\n",
    "        self.n_numbers = n_numbers\n",
    "\n",
    "    def forward(self, state):\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        output = self.network(state)\n",
    "        mean = output[:, :self.n_numbers]\n",
    "        log_std = output[:, self.n_numbers:]\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        std = log_std.exp()\n",
    "        return mean, std\n",
    "\n",
    "    def get_action_and_log_prob(self, state):\n",
    "        mean, std = self.forward(state)\n",
    "        \n",
    "        dist = distributions.Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "    def get_log_prob(self, state, action):\n",
    "        mean, std = self.forward(state)\n",
    "        \n",
    "        if action.dim() != mean.dim():\n",
    "            action = action.view(mean.shape)\n",
    "            \n",
    "        dist = distributions.Normal(mean, std)\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        return log_prob\n",
    "\n",
    "    def get_entropy(self, state):\n",
    "        mean, std = self.forward(state)\n",
    "        dist = distributions.Normal(mean, std)\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "        return entropy\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        return self.network(state)\n",
    "\n",
    "class PPOBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "\n",
    "    def store(self, state, action, log_prob, reward, value, done, next_state):\n",
    "        self.states.append(state.clone())\n",
    "        self.actions.append(action.clone())\n",
    "        self.log_probs.append(log_prob.clone())\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value.clone())\n",
    "        self.dones.append(done)\n",
    "        self.next_states.append(next_state.clone())\n",
    "\n",
    "    def get(self):\n",
    "        states = torch.stack(self.states)\n",
    "        actions = torch.stack(self.actions)\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        rewards = torch.tensor(self.rewards, dtype=torch.float32)\n",
    "        values = torch.stack(self.values).squeeze(-1)\n",
    "        dones = torch.tensor(self.dones, dtype=torch.float32)\n",
    "        next_states = torch.stack(self.next_states)\n",
    "        \n",
    "        return states, actions, log_probs, rewards, values, dones, next_states\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def clear(self):\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "# Simple copy needs to be adjusted \n",
    "class qCritic(nn.Module):\n",
    "    def __init__(self, state_dim, n_qubits=None, n_layers=3):\n",
    "        super(qCritic, self).__init__()\n",
    "        \n",
    "        self.n_qubits = n_qubits if n_qubits is not None else state_dim\n",
    "        self.input_layer = nn.Linear(state_dim, self.n_qubits)\n",
    "        self.dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        \n",
    "        @qml.qnode(self.dev, interface=\"torch\")\n",
    "        def quantum_circuit(inputs, weights):\n",
    "            qml.AngleEmbedding(inputs, wires=range(self.n_qubits)) # encode via angle embedding\n",
    "            \n",
    "            # Variational quantum circuit with n_layers\n",
    "            for layer in range(n_layers):\n",
    "                for qubit in range(self.n_qubits):\n",
    "                    qml.RX(weights[layer, qubit, 0], wires=qubit)\n",
    "                    qml.RY(weights[layer, qubit, 1], wires=qubit)\n",
    "                    qml.RZ(weights[layer, qubit, 2], wires=qubit)\n",
    "                for qubit in range(self.n_qubits - 1):\n",
    "                    qml.CNOT(wires=[qubit, qubit + 1])\n",
    "            \n",
    "            return [qml.expval(qml.PauliZ(w)) for w in range(self.n_qubits)]\n",
    "        \n",
    "        # Initialize quantum weights (n_layers x n_qubits x 3 rotations per qubit)\n",
    "        weight_shapes = {\"weights\": (n_layers, self.n_qubits, 3)}\n",
    "        self.quantum_layer = qml.qnn.TorchLayer(quantum_circuit, weight_shapes)\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.n_qubits, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0) # Add batch dimension if needed\n",
    "        \n",
    "        x = self.relu(self.input_layer(state))\n",
    "        x = self.quantum_layer(x)\n",
    "        value = self.output_layer(x)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c287ae-edeb-4116-aa32-7bd9cfceeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppo implementation \n",
    "#https://verl.readthedocs.io/en/latest/algo/ppo.html\n",
    "\n",
    "def test_agent(step, state, action, log_prob, value, external_eval_func, device, state_dim):\n",
    "        print(f\"Step {step}\")\n",
    "        try:\n",
    "            reward, next_state, _ = external_eval_func(action, step, plot_foil=True)\n",
    "            \n",
    "            if next_state.dim() == 2 and next_state.shape[0] == 1:\n",
    "                next_state = next_state.squeeze(0)\n",
    "            \n",
    "            if next_state.shape[0] != state_dim:\n",
    "                next_state = next_state[:state_dim]\n",
    "            \n",
    "            next_state = next_state.to(device)\n",
    "            \n",
    "            # Return the transition data instead of storing in buffer\n",
    "            return {\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'log_prob': log_prob,\n",
    "                'reward': reward.item(),\n",
    "                'value': value,\n",
    "                'done': 0.0,\n",
    "                'next_state': next_state\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in external_eval at step {step}: {e}\")\n",
    "            print(f\"Action shape: {action.shape}, Action: {action}\")\n",
    "            return None  # Or handle as needed\n",
    "\n",
    "\n",
    "def compute_gae(rewards, values, next_values, dones, gamma=0.99, lam=0.95): #Generalized Advantage Estimation -> computes adventages values\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    gae = 0\n",
    "    \n",
    "    for i in reversed(range(len(rewards))):\n",
    "        if i == len(rewards) - 1:\n",
    "            next_value = next_values[-1]\n",
    "        else:\n",
    "            next_value = values[i + 1]\n",
    "            \n",
    "        delta = rewards[i] + gamma * next_value * (1 - dones[i]) - values[i]\n",
    "        gae = delta + gamma * lam * (1 - dones[i]) * gae\n",
    "        advantages[i] = gae\n",
    "    \n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "#group policy optimizaion instead of ppo?\n",
    "def train_ppo(actor, critic,\n",
    "              external_eval_func, n_numbers=4, state_dim=2, num_episodes=1000, \n",
    "              steps_per_episode=100, lr_actor=3e-4, lr_critic=1e-4, gamma=0.99, \n",
    "              lam=0.95, clip_ratio=0.2, ppo_epochs=4, batch_size=512, \n",
    "              value_loss_coef=0.5, entropy_coef=0.01):\n",
    "    \n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=lr_actor)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=lr_critic)\n",
    "    \n",
    "    # State init  - here simple\n",
    "    state = torch.tensor([0.0], dtype=torch.float32).to(device)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"\\nEpisode {episode + 1}/{num_episodes}\")\n",
    "        \n",
    "        buffer = PPOBuffer(steps_per_episode)\n",
    "\n",
    "        #needs to be revised !    \n",
    "        print(20*\"=\")\n",
    "        \n",
    "        actions_per_ep = []\n",
    "        log_prob_per_ep = []\n",
    "        values_per_ep = []\n",
    "        ids = [get_id(glock) for _ in range(steps_per_episode)]\n",
    "        \n",
    "        for _ in range(steps_per_episode):\n",
    "            with torch.no_grad():\n",
    "                action, log_prob = actor.get_action_and_log_prob(state)\n",
    "                value = critic(state)\n",
    "            actions_per_ep.append(action)\n",
    "            log_prob_per_ep.append(log_prob)\n",
    "            values_per_ep.append(value)\n",
    "        \n",
    "        with Pool(processes=steps_per_episode) as pool: # acting state -> pred -> state -> pred\n",
    "            args = [(step, state, action, log_prob, value, external_eval_func, device, state_dim) \n",
    "                    for step,action, log_prob, value in zip(ids, actions_per_ep, log_prob_per_ep,values_per_ep)]\n",
    "            results = pool.starmap(test_agent, args)\n",
    "        \n",
    "            # Sequentially store valid results in buffer\n",
    "            for result in results:\n",
    "                if result is not None:\n",
    "                    buffer.store(**result)\n",
    "            \n",
    "        \n",
    "        if buffer.size() == 0:\n",
    "            continue\n",
    "            \n",
    "        states, actions, old_log_probs, rewards, values, dones, next_states = buffer.get()\n",
    "        \n",
    "        index = np.argmax(rewards)\n",
    "        print(f\"Best reward: {actions[index]} - {rewards[index]}\")\n",
    "        #external_eval_func(actions[index], get_id(glock), plot_foil=True)\n",
    "\n",
    "        \n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device) \n",
    "        old_log_probs = old_log_probs.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        values = values.to(device)\n",
    "        dones = dones.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_values = critic(next_states).squeeze(-1)\n",
    "        \n",
    "        advantages, returns = compute_gae(rewards, values, next_values, dones, gamma, lam)\n",
    "        advantages = advantages.to(device)\n",
    "        returns = returns.to(device)\n",
    "        \n",
    "        if advantages.std() > 1e-8:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        dataset_size = buffer.size()\n",
    "        \n",
    "        for ppo_epoch in range(ppo_epochs):\n",
    "            indices = torch.randperm(dataset_size)\n",
    "            \n",
    "            for start_idx in range(0, dataset_size, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, dataset_size)\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                \n",
    "                new_log_probs = actor.get_log_prob(batch_states, batch_actions)\n",
    "                new_values = critic(batch_states).squeeze(-1)\n",
    "                \n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                entropy = actor.get_entropy(batch_states).mean()\n",
    "                policy_loss = policy_loss - entropy_coef * entropy\n",
    "                \n",
    "                value_loss = nn.MSELoss()(new_values, batch_returns)\n",
    "                \n",
    "                actor_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(actor.parameters(), 0.5)\n",
    "                actor_optimizer.step()\n",
    "                \n",
    "                critic_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(critic.parameters(), 0.5)\n",
    "                critic_optimizer.step()\n",
    "        \n",
    "        # Log episode results\n",
    "        episode_reward = rewards.median().item()\n",
    "        episode_rewards.append(episode_reward)\n",
    "        if episode>0 and episode%10==0:\n",
    "            print(f\"Episode {episode + 1} - Total Reward: {episode_reward:.4f}\")\n",
    "            print(f\"  Mean Reward: {rewards.mean().item():.4f}\")\n",
    "            print(f\"  Policy Loss: {policy_loss.item():.4f}\")\n",
    "            print(f\"  Value Loss: {value_loss.item():.4f}\")\n",
    "            print(f\"  Entropy: {entropy.item():.4f}\")\n",
    "        \n",
    "        # Print running statistics\n",
    "        if len(episode_rewards) >= 10:\n",
    "            recent_mean = np.mean(episode_rewards[-10:])\n",
    "            print(f\"  Recent 10 episodes average: {recent_mean:.4f}\")\n",
    "    \n",
    "    return actor, critic, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45a3d29-0a2f-4210-b618-5bc0ef3bb108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#needs to have a larger number of \n",
    "n_numbers=4 \n",
    "state_dim=1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "    \n",
    "# Initialize networks\n",
    "actor = Actor(state_dim, n_numbers).to(device)\n",
    "critic = Critic(state_dim).to(device)\n",
    "\n",
    "\n",
    "actor, critic, rewards = train_ppo(actor, critic,\n",
    "            external_eval_func=external_eval,  \n",
    "            n_numbers=1,\n",
    "            num_episodes=50,  \n",
    "            steps_per_episode=4  \n",
    "        )\n",
    "    \n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Final rewards: {rewards[-5:]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0880c-ba12-4f52-a7ae-173813939b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.xlabel(\"Iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d71af5-9faa-43a3-a880-97efeaa994df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#needs to have a larger number of \n",
    "n_numbers=4 \n",
    "state_dim=1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "    \n",
    "# Initialize networks\n",
    "actor = Actor(state_dim, n_numbers).to(device)\n",
    "critic = qCritic(state_dim).to(device)\n",
    "\n",
    "\n",
    "actor, critic, rewards = train_ppo(actor, critic,\n",
    "            external_eval_func=external_eval,  \n",
    "            n_numbers=1,\n",
    "            num_episodes=50,  \n",
    "            steps_per_episode=4  \n",
    "        )\n",
    "    \n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Final rewards: {rewards[-5:]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa35884",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.xlabel(\"Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19437ed",
   "metadata": {},
   "source": [
    "## Result:\n",
    "- Baseline loss: 0.044501\n",
    "- Best loss s_ppo: 0.038491\n",
    "- Best loss q_ppo: 0.038025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da463ca0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actor, critic, rewards2 = train_ppo(actor, critic,\n",
    "            external_eval_func=external_eval,  \n",
    "            n_numbers=1,\n",
    "            num_episodes=100,  \n",
    "            steps_per_episode=16  \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_eval_no(actions, run_id, plot_foil=True):\n",
    "    actions_str = json.dumps(actions.tolist())    \n",
    "    plot_foil_str = 'true' if plot_foil else 'false'\n",
    "    \n",
    "process = subprocess.Popen(\n",
    "        ['python', 'uptake/external_eval.py', actions_str, str(run_id), plot_foil_str],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    stdout, stderr = process.communicate()\n",
    "    print(stdout)\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Subprocess failed: {stderr}\")\n",
    "        return 9999\n",
    "    \n",
    "    try:\n",
    "        return json.loads(stdout.strip())\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid output: {stdout}\")\n",
    "        return 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    1: [\"camber_line\",\"control_points_params\",\"P1_k_factor\",],\n",
    "    2: [\"camber_line\",\"control_points_params\",\"P3_k_factor\",(-0.3,-0.1)],\n",
    "    3: [\"top_thickness\",\"control_points_params\",\"P1_k_factor\",(0.15,0.4)],\n",
    "    4: [\"top_thickness\",\"le_y_thickness\",(0.01,0.04)]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "err_log_name = 'coeff_test_1.csv'\n",
    "seed = 1\n",
    "cpu_count = 30\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "counter = Value('i', 0)\n",
    "counter_lock = Lock()\n",
    "\n",
    "\n",
    "def to_log(elem, uuid, err):\n",
    "    with counter_lock:\n",
    "        print(elem[0])\n",
    "        try:\n",
    "            log = {\n",
    "                \"ID\": uuid,\n",
    "                \"1\": f\"{elem[0]:.5f}\" ,\n",
    "                \"2\": f\"{elem[1]:.5f}\",\n",
    "                \"3\": f\"{elem[2]:.5f}\",\n",
    "                \"4\": f\"{elem[3]:.5f}\",\n",
    "                \"Error\": err\n",
    "            }\n",
    "            pd.DataFrame(log, index=[0]).to_csv(err_log_name, header=not os.path.exists(err_log_name), index=False, mode='a')\n",
    "        except Exception as e:\n",
    "            print(f\"Ex: {e}\")\n",
    "        print(\"Save done\")\n",
    "\n",
    "def create_uid_():\n",
    "    with counter_lock:\n",
    "        ret_val = '0' * (5 - len(str(counter.value))) + str(counter.value)\n",
    "        counter.value += 1\n",
    "    return ret_val\n",
    "\n",
    "def evaluate(individual):\n",
    "    uuid_ = create_uid_()\n",
    "    err = external_eval_no(individual,uuid_)\n",
    "    to_log(individual, uuid_, err)\n",
    "    return err, uuid_\n",
    "\n",
    "def objective_function(x):\n",
    "    result, uuid = evaluate(x)\n",
    "    str_list = \";\".join([f\"{elem:.4f}\" for elem in x])\n",
    "    os.system(f\"echo '{uuid};{result};{str_list}'>>log_file.csv\")\n",
    "    return result\n",
    "\n",
    "# Define bounds for the parameters\n",
    "bounds = [(0.1,0.5), (-0.4,-0.1), (0.15,0.5), (0.01,0.05)]  # Example bounds\n",
    "\n",
    "result = differential_evolution(objective_function, bounds, strategy='best1bin', maxiter=100, popsize=30, tol=0.01, mutation=(0.5, 1), recombination=0.5, workers=cpu_count)\n",
    "\n",
    "print(\"Best cost:\", result.fun)\n",
    "print(\"Best position:\", result.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3676f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c29489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "phd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
