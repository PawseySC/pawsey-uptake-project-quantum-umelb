{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d82bbe3",
   "metadata": {},
   "source": [
    "# Airfoil optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e81e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import distributions\n",
    "from torch.distributions import Normal\n",
    "\n",
    "#stuff for neurofoil - pip install neuralfoil\n",
    "from aerosandbox.geometry.airfoil.airfoil_families import get_kulfan_parameters, get_kulfan_coordinates\n",
    "import aerosandbox as asb\n",
    "import aerosandbox.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import aerosandbox.tools.pretty_plots as p\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import aerosandbox.tools.pretty_plots as p\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import contextlib\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "import pennylane as qml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94ef23",
   "metadata": {},
   "source": [
    "## Procedure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df10d0d",
   "metadata": {},
   "source": [
    "Sources:\n",
    "* http://servidor.demec.ufpr.br/CFD/bibliografia/aerodinamica/kulfan_2007.pdf\n",
    "* https://www.tandfonline.com/doi/epdf/10.1080/19942060.2024.2445144\n",
    "* https://github.com/peterdsharpe/NeuralFoil\n",
    "\n",
    "\n",
    "Here the PINN is allready trained an provides the prediction. Therefore, the idea is, we shift the quantum opt. part to approximate a policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db3dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SVG(filename=\"optimprocedure.svg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3325b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"../blade_design_tools\"))\n",
    "import create_airfoil as ca\n",
    "airfoil_top_points, airfoil_bottom_points, camber_curve_points = ca.generate_shape(config_filepath=\"../blade_design_tools/airfoil_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b855ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neuralfoil_coordinates(airfoil_top_points, airfoil_bottom_points):\n",
    "    \"\"\"\n",
    "        stacking is up for neurofoil\n",
    "    \"\"\"\n",
    "    # Upper surface: reverse to go from TE to LE\n",
    "    upper_te_to_le = np.flip(airfoil_top_points, axis=0)\n",
    "\n",
    "    # Lower surface: LE to TE as is\n",
    "    lower_le_to_te = airfoil_bottom_points\n",
    "\n",
    "    # Check if LE points match (for sharp LE); avoid duplicating if so\n",
    "    if np.allclose(upper_te_to_le[-1], lower_le_to_te[0]):\n",
    "        coordinates = np.vstack((upper_te_to_le, lower_le_to_te[1:]))\n",
    "    else:\n",
    "        coordinates = np.vstack((upper_te_to_le, lower_le_to_te))\n",
    "    return coordinates\n",
    "\n",
    "\n",
    "# 7. Export for NeuralFoil\n",
    "coordinates = get_neuralfoil_coordinates(airfoil_top_points, airfoil_bottom_points)\n",
    "kulfan_params = get_kulfan_parameters(coordinates)\n",
    "print(\"Kulfan parameters:\", kulfan_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d8a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "CL_multipoint_targets = np.array([0.8, 1.0, 1.2, 1.4, 1.5, 1.6])\n",
    "CL_multipoint_weights = np.array([5, 6, 7, 8, 9, 10])\n",
    "\n",
    "Re = 500e3 * (CL_multipoint_targets / 1.25) ** -0.5\n",
    "mach = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf01462-6d96-44c8-b6a0-3e6eb604c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9764e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_state(initial_guess_airfoil):\n",
    "    opti_ = asb.Opti()\n",
    "    optimized_airfoil_ = asb.KulfanAirfoil(\n",
    "        name=\"Optimized\",\n",
    "        lower_weights=opti_.variable(\n",
    "            init_guess=initial_guess_airfoil.lower_weights,\n",
    "            lower_bound=-0.5,\n",
    "            upper_bound=0.25,\n",
    "        ),\n",
    "        upper_weights=opti_.variable(\n",
    "            init_guess=initial_guess_airfoil.upper_weights,\n",
    "            lower_bound=-0.25,\n",
    "            upper_bound=0.5,\n",
    "        ),\n",
    "        leading_edge_weight=opti_.variable(\n",
    "            init_guess=initial_guess_airfoil.leading_edge_weight,\n",
    "            lower_bound=-1,\n",
    "            upper_bound=1,\n",
    "        ),\n",
    "        TE_thickness=kulfan_params[\"TE_thickness\"],\n",
    "    )\n",
    "\n",
    "    alpha_ = opti_.variable(\n",
    "        init_guess=45\n",
    "    )\n",
    "\n",
    "\n",
    "    aero_ = optimized_airfoil_.get_aero_from_neuralfoil(\n",
    "        alpha=alpha_,\n",
    "        Re=Re,\n",
    "        mach=mach,\n",
    "    )\n",
    "    return aero_, optimized_airfoil_, alpha_, opti_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37a14f3",
   "metadata": {},
   "source": [
    "# Test optimization frame work\n",
    "### TODO's: \n",
    "- Implement real costfunction\n",
    "- Shape constraints\n",
    "- Implement the PPO, here we compare standard PPO vs. Quantum PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0e17a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optim(opti_, aero_, target_, initial_guess, alpha_):\n",
    "    opti_.subject_to([\n",
    "        aero_[\"analysis_confidence\"] > 0.90,\n",
    "        aero_[\"CL\"] == CL_multipoint_targets,\n",
    "        np.diff(alpha_) > 0,\n",
    "        aero_[\"CM\"] >= -0.133,\n",
    "        target_.local_thickness(x_over_c=0.33) >= 0.128,\n",
    "        target_.local_thickness(x_over_c=0.90) >= 0.014,\n",
    "        target_.TE_angle() >= 6.03, \n",
    "        target_.lower_weights[0] < -0.05,\n",
    "        target_.upper_weights[0] > 0.05,\n",
    "        target_.local_thickness() > 0\n",
    "    ])\n",
    "    \n",
    "    get_wiggliness = lambda af: sum([\n",
    "        np.sum(np.diff(np.diff(array)) ** 2)\n",
    "        for array in [af.lower_weights, af.upper_weights]\n",
    "    ])\n",
    "    \n",
    "    opti_.subject_to(\n",
    "        get_wiggliness(target_) < 2 * get_wiggliness(initial_guess)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Formulate an optimization goal based\n",
    "    opti_.maximize(np.mean(aero_[\"CL\"] / aero_[\"CD\"]))\n",
    "    \n",
    "    sol = opti_.solve(\n",
    "        max_iter = 10,\n",
    "        behavior_on_failure=\"return_last\",\n",
    "        options={\n",
    "           # \"ipopt.mu_strategy\": 'monotone',\n",
    "           # \"ipopt.start_with_resto\": 'yes'\n",
    "        }\n",
    "    )\n",
    "    return sol\n",
    "\n",
    "#optimized_airfoil_0 = sol(optimized_airfoil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c5959e",
   "metadata": {},
   "source": [
    "### Interface for NeuralFoil\n",
    "This provides the values employed for guiding the optimization, here the method call for `get_aero_from_neuralfoil` yields the following values. Question: how to use this parameter to approximate our loss value sought? \n",
    "\n",
    "```python\n",
    "return {\n",
    "            \"analysis_confidence\": nf_aero[\"analysis_confidence\"],\n",
    "            \"CL\": CL,\n",
    "            \"CD\": CD,\n",
    "            \"CM\": CM,\n",
    "            \"Cpmin\": Cpmin,\n",
    "            \"Top_Xtr\": Top_Xtr,\n",
    "            \"Bot_Xtr\": Bot_Xtr,\n",
    "            \"mach_crit\": mach_crit,\n",
    "            \"mach_dd\": mach_dd,\n",
    "            \"Cpmin_0\": Cpmin_0,\n",
    "            **{f\"upper_bl_theta_{i}\": nf_aero[f\"upper_bl_theta_{i}\"] for i in range(N)},\n",
    "            **{f\"upper_bl_H_{i}\": nf_aero[f\"upper_bl_H_{i}\"] for i in range(N)},\n",
    "            **{\n",
    "                f\"upper_bl_ue/vinf_{i}\": nf_aero[f\"upper_bl_ue/vinf_{i}\"]\n",
    "                for i in range(N)\n",
    "            },\n",
    "            **{f\"lower_bl_theta_{i}\": nf_aero[f\"lower_bl_theta_{i}\"] for i in range(N)},\n",
    "            **{f\"lower_bl_H_{i}\": nf_aero[f\"lower_bl_H_{i}\"] for i in range(N)},\n",
    "            **{\n",
    "                f\"lower_bl_ue/vinf_{i}\": nf_aero[f\"lower_bl_ue/vinf_{i}\"]\n",
    "                for i in range(N)\n",
    "            },\n",
    "        }\n",
    "```\n",
    "\n",
    "Hint the returns values such as CL are wrapped into Casadi type. Therefore we need to employ solve to evaluate the AD graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98572a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified shape_method\n",
    "def generate_shape(config_filepath=\"airfoil_config.yaml\", config_dict=None, plot_foil=False):\n",
    "    try:\n",
    "        config = ca.load_config(config_filepath) if config_dict is None else config_dict\n",
    "        \n",
    "        # Handle optional input targets for chord and max thickness\n",
    "        input_targets = config.get('input_targets', {})\n",
    "        desired_chord = input_targets.get('desired_chord_length', None)\n",
    "        desired_max_th = input_targets.get('desired_max_thickness', None)\n",
    "        \n",
    "        # Temporarily compute camber to get normalized chord (needed for adjustments)\n",
    "        temp_camber_control_points, temp_camber_curve_points = ca.compute_curve_points(\n",
    "            config['camber_line'], \"camber\", config['output_settings']['num_points_on_curve']\n",
    "        )\n",
    "        norm_le = temp_camber_curve_points[0]\n",
    "        norm_te = temp_camber_curve_points[-1]\n",
    "        norm_chord = np.linalg.norm(norm_te - norm_le)\n",
    "        norm_x_span = norm_te[0] - norm_le[0]  # Should be 1.0\n",
    "        \n",
    "        # Adjust chord_length_for_export if desired_chord is set (to achieve desired geometric chord)\n",
    "        if desired_chord is not None:\n",
    "            config['output_settings']['chord_length_for_export'] = desired_chord * (norm_x_span / norm_chord)  # Equivalent to desired_chord * np.cos(np.deg2rad(np.abs(config['camber_line']['stagger_angle_deg'])))\n",
    "        \n",
    "        # 1. Compute Camber Line\n",
    "        camber_control_points, camber_curve_points = ca.compute_curve_points(\n",
    "            config['camber_line'], \"camber\", config['output_settings']['num_points_on_curve']\n",
    "        )\n",
    "       \n",
    "        # 2. Compute Top Thickness Distribution\n",
    "        top_thickness_control_points, top_thickness_curve_points = ca.compute_curve_points(\n",
    "            config['top_thickness'], \"top_thickness\", config['output_settings']['num_points_on_curve']\n",
    "        )\n",
    "        # 3. Compute Bottom Thickness Distribution\n",
    "        bottom_thickness_control_points, bottom_thickness_curve_points = ca.compute_curve_points(\n",
    "            config['bottom_thickness'], \"bottom_thickness\", config['output_settings']['num_points_on_curve']\n",
    "        )\n",
    "        \n",
    "        # Temporarily compute current normalized max thickness\n",
    "        current_norm_thickness_along = top_thickness_curve_points[:, 1] + bottom_thickness_curve_points[:, 1]\n",
    "        current_norm_max_th = np.max(current_norm_thickness_along)\n",
    "        \n",
    "        # Compute scale factor (reflects any chord adjustment)\n",
    "        scale = config['output_settings']['chord_length_for_export'] / norm_x_span\n",
    "        \n",
    "        # Adjust thickness curves if desired_max_th is set\n",
    "        if desired_max_th is not None:\n",
    "            desired_norm_max_th = desired_max_th / scale\n",
    "            th_scale_factor = desired_norm_max_th / current_norm_max_th\n",
    "            top_thickness_curve_points[:, 1] *= th_scale_factor\n",
    "            bottom_thickness_curve_points[:, 1] *= th_scale_factor\n",
    "        \n",
    "        # 4. Generate Full Airfoil Contour\n",
    "        airfoil_top_points, airfoil_bottom_points = ca.generate_airfoil_contour(\n",
    "            camber_curve_points,\n",
    "            top_thickness_curve_points,\n",
    "            bottom_thickness_curve_points,\n",
    "            config['camber_line']['inlet_angle_deg'],\n",
    "            config['output_settings']['num_points_on_arc']\n",
    "        )\n",
    "        # 5. Plot the Airfoil\n",
    "        if plot_foil:\n",
    "            ca.plot_airfoil(camber_curve_points, airfoil_top_points, airfoil_bottom_points,\n",
    "                         config['output_settings']['plot_title'])\n",
    "    except:\n",
    "        pass\n",
    "       \n",
    "    return airfoil_top_points, airfoil_bottom_points, camber_curve_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with to adjustable params:\n",
    "# NetworkA -> ChordLength + MaxThickness - using black box function to translate this into our parametrization\n",
    "config = ca.load_config(\"../blade_design_tools/airfoil_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d3a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config['input_targets'] = {\n",
    "#    \"desired_chord_length\" : 1.0,\n",
    "#    \"desired_max_thickness\": 0.05\n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12575eb1-3392-4421-85d2-ece281dddf11",
   "metadata": {},
   "source": [
    "# Actor crtitic training\n",
    "- For the purpose of a prototype, we start modifying a few control points\n",
    "```python\n",
    "camber_line:\n",
    "  control_points_params:\n",
    "    P1_k_factor: 0.4\n",
    "    P2_x: 0.5\n",
    "    P2_y: 0.05\n",
    "    P3_k_factor: -0.25\n",
    "  inlet_angle_deg: 45\n",
    "  outlet_angle_deg: -50\n",
    "  stagger_angle_deg: -25\n",
    "\n",
    "top_thickness:\n",
    "  control_points_params:\n",
    "    P1_k_factor: 0.2\n",
    "    P2_x: 0.1\n",
    "    P2_y: 0.03\n",
    "    P3_k_factor: 0\n",
    "  inlet_angle_deg: 40\n",
    "  outlet_angle_deg: -0\n",
    "  le_y_thickness: 0.02 # Y-coordinate of P0 for thickness curve (half-thickness at LE)\n",
    "  p4_y: 0.01           # Y-coordinate of P4 for thickness curve (half-thickness at TE)\n",
    "\n",
    "bottom_thickness:\n",
    "  control_points_params:\n",
    "    P1_k_factor: 0.2\n",
    "    P2_x: 0.1\n",
    "    P2_y: 0.03\n",
    "    P3_k_factor: 0\n",
    "  inlet_angle_deg: 40\n",
    "  outlet_angle_deg: -0\n",
    "  le_y_thickness: 0.02 # Y-coordinate of P0 for thickness curve (half-thickness at LE)\n",
    "  p4_y: 0.01           # Y-coordinate of P4 for thickness curve (half-thickness at TE)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c3dca-e834-4364-8963-c3966850f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def external_eval(actions, plot_foil=False):  # actions might be given as numpy\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        with contextlib.redirect_stdout(devnull):\n",
    "            actions_ = actions.numpy()[0]\n",
    "            fail_ret_val = torch.tensor([-1]), actions\n",
    "            if actions_[0]<actions_[2]:\n",
    "                return fail_ret_val\n",
    "            \n",
    "            print(actions)\n",
    "            c_p1_lower_bound = 0.15\n",
    "            c_p3_lower_bound = -0.3\n",
    "            t_p1_k_lower_bound = 0.15\n",
    "            t_le_y_thick_lower_bound = 0.01\n",
    " \n",
    "            c_p1_upper_bound = 0.5\n",
    "            c_p3_upper_bound = -0.1\n",
    "            t_p1_k_upper_bound  = 0.4\n",
    "            t_le_y_thick_upper_bound = 0.04\n",
    "            \n",
    "            config[\"camber_line\"][\"control_points_params\"][\"P1_k_factor\"] = c_p1_lower_bound + actions_[0] * (c_p1_upper_bound - c_p1_lower_bound)\n",
    "            config[\"camber_line\"][\"control_points_params\"][\"P3_k_factor\"] = c_p3_lower_bound + actions_[1] * (c_p3_upper_bound - (c_p3_lower_bound))\n",
    "            \n",
    "            # Scale and assign top_thickness parameters\n",
    "            config[\"top_thickness\"][\"control_points_params\"][\"P1_k_factor\"] = t_p1_k_lower_bound + actions_[2] * (t_p1_k_upper_bound - t_p1_k_lower_bound)\n",
    "            config[\"top_thickness\"][\"le_y_thickness\"] = t_le_y_thick_lower_bound + actions_[3] * (t_le_y_thick_upper_bound - t_le_y_thick_lower_bound)\n",
    "           \n",
    "            # parse actions to generate shape\n",
    "            airfoil_top_points, airfoil_bottom_points, _ = generate_shape(config_dict=config, plot_foil=plot_foil)\n",
    "       \n",
    "            # transform shape -> later we replace this with the fortran solver\n",
    "            coordinates = get_neuralfoil_coordinates(airfoil_top_points, airfoil_bottom_points)\n",
    "            kulfan_params = get_kulfan_parameters(coordinates)\n",
    "            kulfan_airfoil_ = asb.KulfanAirfoil(name=\"State 1'\", **kulfan_params)\n",
    "           \n",
    "            # parametrize the optimization states:\n",
    "            aero_new, optimized_airfoil_new, alpha_new, opti_new = opt_state(kulfan_airfoil_)\n",
    "            try:\n",
    "                sol = get_optim(opti_new, aero_new, optimized_airfoil_new, kulfan_airfoil_, alpha_new)\n",
    "                resolved_values_0 = sol(aero_new)\n",
    "            except:\n",
    "               return torch.tensor([0]), actions\n",
    "            ratio = torch.tensor(resolved_values_0[\"CL\"] / resolved_values_0[\"CD\"])\n",
    "            # define a cost value\n",
    "    return torch.mean(ratio), torch.tensor([[np.mean(resolved_values_0[\"CL\"]), np.mean(resolved_values_0[\"CL\"])]], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56273ea-8ccf-46b1-a9a9-fd9961fc9a00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "source": [
    "# First attempt - simple AC with a vanilla policy opt. \n",
    "\n",
    "```python\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_numbers, hidden_dim=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),  # Simple state input (can be modified)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_numbers * 2)  # Mean and log_std for each number\n",
    "        )\n",
    "        self.n_numbers = n_numbers\n",
    "    def forward(self, state):\n",
    "        params = self.network(state)\n",
    "        mean, log_std = params[:, :self.n_numbers], params[:, self.n_numbers:]\n",
    "        std = log_std.exp()\n",
    "        dist = distributions.Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        return action, log_prob\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_numbers, hidden_dim=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(2 + n_numbers, hidden_dim),  # State (dim 2) + action\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Value estimate\n",
    "        )\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.network(x)\n",
    "\n",
    "def train_actor_critic(n_numbers, num_episodes=1000, lr_actor=1e-4, lr_critic=1e-4, gamma=0.90):\n",
    "    actor = Actor(n_numbers)\n",
    "    critic = Critic(n_numbers)\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=lr_actor)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=lr_critic)\n",
    "    plot_foil = False\n",
    "    state = torch.tensor([[1.0, 1.0]], dtype=torch.float32)\n",
    "    for episode in range(num_episodes):\n",
    "        action, log_prob = actor(state)\n",
    "        reward, state = external_eval(action, plot_foil)\n",
    "        # Critic estimates value\n",
    "        value = critic(state, action)\n",
    "        # Compute advantage\n",
    "        advantage = reward - value.detach()\n",
    "        # Critic loss: Mean squared error\n",
    "        critic_loss = (reward - value).pow(2).mean()\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "        # Actor loss: Policy gradient\n",
    "        actor_loss = -(log_prob * advantage).mean()\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {reward.item():.4f}, Actor Loss: {actor_loss.item():.4f}, Critic Loss: {critic_loss.item():.4f}\")\n",
    "            plot_foil = True\n",
    "        else:\n",
    "            plot_foil = False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_numbers = 4\n",
    "    train_actor_critic(n_numbers)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fa464-0692-4fe7-b97d-87e890e6d25e",
   "metadata": {},
   "source": [
    "# Refine AC with PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc117e-6540-4561-8333-aa9f5b50636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Networks\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, n_numbers, hidden_dim=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_numbers * 2)\n",
    "        )\n",
    "        self.n_numbers = n_numbers\n",
    "\n",
    "    def forward(self, state):\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        output = self.network(state)\n",
    "        mean = output[:, :self.n_numbers]\n",
    "        log_std = output[:, self.n_numbers:]\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        std = log_std.exp()\n",
    "        return mean, std\n",
    "\n",
    "    def get_action_and_log_prob(self, state):\n",
    "        mean, std = self.forward(state)\n",
    "        \n",
    "        dist = distributions.Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "    def get_log_prob(self, state, action):\n",
    "        mean, std = self.forward(state)\n",
    "        \n",
    "        if action.dim() != mean.dim():\n",
    "            action = action.view(mean.shape)\n",
    "            \n",
    "        dist = distributions.Normal(mean, std)\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        return log_prob\n",
    "\n",
    "    def get_entropy(self, state):\n",
    "        mean, std = self.forward(state)\n",
    "        dist = distributions.Normal(mean, std)\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "        return entropy\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        return self.network(state)\n",
    "\n",
    "class PPOBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.next_states = []\n",
    "\n",
    "    def store(self, state, action, log_prob, reward, value, done, next_state):\n",
    "        self.states.append(state.clone())\n",
    "        self.actions.append(action.clone())\n",
    "        self.log_probs.append(log_prob.clone())\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value.clone())\n",
    "        self.dones.append(done)\n",
    "        self.next_states.append(next_state.clone())\n",
    "\n",
    "    def get(self):\n",
    "        states = torch.stack(self.states)\n",
    "        actions = torch.stack(self.actions)\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        rewards = torch.tensor(self.rewards, dtype=torch.float32)\n",
    "        values = torch.stack(self.values).squeeze(-1)\n",
    "        dones = torch.tensor(self.dones, dtype=torch.float32)\n",
    "        next_states = torch.stack(self.next_states)\n",
    "        \n",
    "        return states, actions, log_probs, rewards, values, dones, next_states\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def clear(self):\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "# Simple copy needs to be adjusted \n",
    "class qCritic(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=2, n_qubits=None, n_layers=3):\n",
    "        super(qCritic, self).__init__()\n",
    "        \n",
    "        self.n_qubits = n_qubits if n_qubits is not None else state_dim\n",
    "        self.input_layer = nn.Linear(state_dim, hidden_dim)\n",
    "        self.dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        \n",
    "        @qml.qnode(self.dev, interface=\"torch\")\n",
    "        def quantum_circuit(inputs, weights):\n",
    "            qml.AngleEmbedding(inputs, wires=range(self.n_qubits)) # encode via angle embedding\n",
    "            \n",
    "            # Variational quantum circuit with n_layers\n",
    "            for layer in range(n_layers):\n",
    "                for qubit in range(self.n_qubits):\n",
    "                    qml.RX(weights[layer, qubit, 0], wires=qubit)\n",
    "                    qml.RY(weights[layer, qubit, 1], wires=qubit)\n",
    "                    qml.RZ(weights[layer, qubit, 2], wires=qubit)\n",
    "                for qubit in range(self.n_qubits - 1):\n",
    "                    qml.CNOT(wires=[qubit, qubit + 1])\n",
    "            \n",
    "            return [qml.expval(qml.PauliZ(w)) for w in range(self.n_qubits)]\n",
    "        \n",
    "        # Initialize quantum weights (n_layers x n_qubits x 3 rotations per qubit)\n",
    "        weight_shapes = {\"weights\": (n_layers, self.n_qubits, 3)}\n",
    "        self.quantum_layer = qml.qnn.TorchLayer(quantum_circuit, weight_shapes)\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.n_qubits, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if state.dim() == 1:\n",
    "            state = state.unsqueeze(0)  # Add batch dimension if needed\n",
    "        \n",
    "        x = self.relu(self.input_layer(state))\n",
    "        x = self.quantum_layer(x)\n",
    "        value = self.output_layer(x)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c287ae-edeb-4116-aa32-7bd9cfceeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ppo implementation \n",
    "#https://verl.readthedocs.io/en/latest/algo/ppo.html\n",
    "\n",
    "def compute_gae(rewards, values, next_values, dones, gamma=0.99, lam=0.95): #Generalized Advantage Estimation -> computes adventages values\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    gae = 0\n",
    "    \n",
    "    for i in reversed(range(len(rewards))):\n",
    "        if i == len(rewards) - 1:\n",
    "            next_value = next_values[-1]\n",
    "        else:\n",
    "            next_value = values[i + 1]\n",
    "            \n",
    "        delta = rewards[i] + gamma * next_value * (1 - dones[i]) - values[i]\n",
    "        gae = delta + gamma * lam * (1 - dones[i]) * gae\n",
    "        advantages[i] = gae\n",
    "    \n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "#group policy optimizaion instead of ppo?\n",
    "def train_ppo(actor, critic,\n",
    "              external_eval_func, n_numbers=4, state_dim=2, num_episodes=1000, \n",
    "              steps_per_episode=100, lr_actor=3e-4, lr_critic=1e-4, gamma=0.99, \n",
    "              lam=0.95, clip_ratio=0.2, ppo_epochs=4, batch_size=512, \n",
    "              value_loss_coef=0.5, entropy_coef=0.01):\n",
    "    \n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=lr_actor)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=lr_critic)\n",
    "    \n",
    "    # State init  - here simple\n",
    "    state = torch.tensor([0.0, 0.0], dtype=torch.float32).to(device)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"\\nEpisode {episode + 1}/{num_episodes}\")\n",
    "        \n",
    "        buffer = PPOBuffer(steps_per_episode)\n",
    "    \n",
    "        #needs to be revised !    \n",
    "        for step in range(steps_per_episode):\n",
    "            with torch.no_grad():\n",
    "                action, log_prob = actor.get_action_and_log_prob(state)\n",
    "                \n",
    "                value = critic(state)\n",
    "                \n",
    "                try:\n",
    "                    reward, next_state = external_eval_func(action, plot_foil=False)\n",
    "                    \n",
    "                    if next_state.dim() == 2 and next_state.shape[0] == 1:\n",
    "                        next_state = next_state.squeeze(0)\n",
    "                    \n",
    "                    # Ensure next_state matches your state_dim\n",
    "                    if next_state.shape[0] != state_dim:\n",
    "                        next_state = next_state[:state_dim]  \n",
    "                    \n",
    "                    next_state = next_state.to(device)\n",
    "                    \n",
    "                    # Store transition\n",
    "                    buffer.store(\n",
    "                        state=state,\n",
    "                        action=action, \n",
    "                        log_prob=log_prob,\n",
    "                        reward=reward.item(),\n",
    "                        value=value,\n",
    "                        done=0.0,  # no episdode end\n",
    "                        next_state=next_state\n",
    "                    )\n",
    "                    state = next_state\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in external_eval at step {step}: {e}\")\n",
    "                    print(f\"Action shape: {action.shape}, Action: {action}\")\n",
    "                    continue\n",
    "        \n",
    "        if buffer.size() == 0:\n",
    "            continue\n",
    "            \n",
    "        states, actions, old_log_probs, rewards, values, dones, next_states = buffer.get()\n",
    "        \n",
    "        index = np.argmax(rewards)\n",
    "        print(f\"Best reward: {actions[index]} - {rewards[index]}\")\n",
    "        external_eval_func(actions[index], plot_foil=True)\n",
    "\n",
    "        \n",
    "        states = states.to(device)\n",
    "        actions = actions.to(device) \n",
    "        old_log_probs = old_log_probs.to(device)\n",
    "        rewards = rewards.to(device)\n",
    "        values = values.to(device)\n",
    "        dones = dones.to(device)\n",
    "        next_states = next_states.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_values = critic(next_states).squeeze(-1)\n",
    "        \n",
    "        advantages, returns = compute_gae(rewards, values, next_values, dones, gamma, lam)\n",
    "        advantages = advantages.to(device)\n",
    "        returns = returns.to(device)\n",
    "        \n",
    "        if advantages.std() > 1e-8:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        dataset_size = buffer.size()\n",
    "        \n",
    "        for ppo_epoch in range(ppo_epochs):\n",
    "            indices = torch.randperm(dataset_size)\n",
    "            \n",
    "            for start_idx in range(0, dataset_size, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, dataset_size)\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                \n",
    "                new_log_probs = actor.get_log_prob(batch_states, batch_actions)\n",
    "                new_values = critic(batch_states).squeeze(-1)\n",
    "                \n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                entropy = actor.get_entropy(batch_states).mean()\n",
    "                policy_loss = policy_loss - entropy_coef * entropy\n",
    "                \n",
    "                value_loss = nn.MSELoss()(new_values, batch_returns)\n",
    "                \n",
    "                actor_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(actor.parameters(), 0.5)\n",
    "                actor_optimizer.step()\n",
    "                \n",
    "                critic_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(critic.parameters(), 0.5)\n",
    "                critic_optimizer.step()\n",
    "        \n",
    "        # Log episode results\n",
    "        episode_reward = rewards.median().item()\n",
    "        episode_rewards.append(episode_reward)\n",
    "        if episode>0 and episode%10==0:\n",
    "            print(f\"Episode {episode + 1} - Total Reward: {episode_reward:.4f}\")\n",
    "            print(f\"  Mean Reward: {rewards.mean().item():.4f}\")\n",
    "            print(f\"  Policy Loss: {policy_loss.item():.4f}\")\n",
    "            print(f\"  Value Loss: {value_loss.item():.4f}\")\n",
    "            print(f\"  Entropy: {entropy.item():.4f}\")\n",
    "        \n",
    "        # Print running statistics\n",
    "        if len(episode_rewards) >= 10:\n",
    "            recent_mean = np.mean(episode_rewards[-10:])\n",
    "            print(f\"  Recent 10 episodes average: {recent_mean:.4f}\")\n",
    "    \n",
    "    return actor, critic, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45a3d29-0a2f-4210-b618-5bc0ef3bb108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs to have a larger number of \n",
    "n_numbers=4 \n",
    "state_dim=2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "    \n",
    "# Initialize networks\n",
    "actor = Actor(state_dim, n_numbers).to(device)\n",
    "critic = Critic(state_dim).to(device)\n",
    "\n",
    "\n",
    "actor, critic, rewards = train_ppo(actor, critic,\n",
    "            external_eval_func=external_eval,  \n",
    "            n_numbers=4,\n",
    "            num_episodes=20,  \n",
    "            steps_per_episode=128  \n",
    "        )\n",
    "    \n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Final rewards: {rewards[-5:]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0880c-ba12-4f52-a7ae-173813939b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.xlabel(\"Iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab9fd11-7d90-435f-913b-b2269190e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs to have a larger number of \n",
    "n_numbers=4 \n",
    "state_dim=2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "    \n",
    "# Initialize networks\n",
    "actor = Actor(state_dim, n_numbers).to(device)\n",
    "critic = qCritic(state_dim).to(device)\n",
    "\n",
    "\n",
    "actor, critic, rewards = train_ppo(actor, critic,\n",
    "            external_eval_func=external_eval,  \n",
    "            n_numbers=4,\n",
    "            num_episodes=20,  \n",
    "            steps_per_episode=128  \n",
    "        )\n",
    "    \n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Final rewards: {rewards[-5:]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9577f5b1-58a3-42de-a568-31db0f4b653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.xlabel(\"Iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaabdda-723c-4a6d-af08-13429ec0b028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odef",
   "language": "python",
   "name": "odef"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
